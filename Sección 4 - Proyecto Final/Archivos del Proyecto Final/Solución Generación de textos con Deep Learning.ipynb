{"cells":[{"cell_type":"markdown","metadata":{"id":"1WjJD6sIGfGA"},"source":["# Generación de textos con Deep Learning\n","\n","Las redes neuronales recurrentes también pueden usarse como modelos generativos. Esto significa que, además de ser utilizados para modelos predictivos (hacer predicciones), pueden aprender las secuencias de un problema y luego generar secuencias plausibles completamente nuevas para el dominio del problema.  En este proyecto, vamos a descubrir cómo crear un modelo generativo de texto, carácter por carácter, utilizando las redes neuronales recurrentes de LSTM en Python con Keras.\n"]},{"cell_type":"markdown","metadata":{"id":"hLXJ2bB6GfGE"},"source":["# **Descripción del problema: Generación de texto**\n","\n","## **¿Qué utilidad tienen los modelos generativos?**\n","Estos modelos permiten en base a un conjunto de datos aprender y generar datos siguiendo secuencias aprendidas bajo los datos. Esto se utiliza mucho en **phising**. En base a mensajes preestablecidos, se generan emails, cuentas de usuarios, mensajes en aplicaciones de mensajería... etc.. para engañar al usuarios simulando que se encuentra en una conversión real. O también  en los **chatbots** para replicar la comunicación humana sin necesidad de tener a una persona al otro lado y facilitar la interacción por ejemplo con una empresa las 24 horas del día.\n","\n","## **Enunciado**\n","Para implementar un modelo generativo sencillo vamos a seleccionar un libro de texto sencillo de nuestra infancia para utilizarlo como base de aprendizaje y a partir de ahí con Redes LSTM generar un modelo que pueda generar un texto a partir de su aprendizaje. Aprenderemos las dependencias entre los caracteres y las probabilidades condicionales de los personajes en las secuencias para que a su vez podamos generar secuencias de caracteres totalmente nuevas y originales.\n","\n","Como datos de la práctica se entregan los siguientes ficheros:\n","- Cuento de Los 3 cerditos que sirva como base de aprendizaje (puedes utilizar el libro o texto que consideres)\n","- El proyecto realizado y explicado como ejemplo con una pequeña red neuronal\n"]},{"cell_type":"markdown","metadata":{"id":"a4icLV9JGfGM"},"source":["# Red Neuronal Recurrente LSTM más grande\n","Ahora vamos ha hacer los mismo creando una red mucho más grande. Mantendremos el mismo número de unidades de memoria en 256, pero añadiremos una segunda capa."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsgS8s1ZGfGM"},"outputs":[],"source":["#Define la LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"]},{"cell_type":"markdown","source":["También cambiaremos el nombre del peso de los puntos de control para que podamos determinar la diferencia entre los pesos de esta red y el anterior (agregando la palabra más grande en el nombre del archivo):\n","\n","**filename=\"pesos-los3grande-303-30-2.3081.hdf5\"**\n","\n","Por último, aumentaremos el número de epoch de formación de 50 a 300 y reduciremos el tamaño del lote de 128 a 64 para dar a la red más oportunidades de actualización y aprendizaje. El código completo sería (con todos los pasos vistos):"],"metadata":{"id":"j_6Fela__vLo"}},{"cell_type":"code","source":["#Grande LSTM Network para generar texto 3 cerditos\n","import sys\n","import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","#Cargamos el texto y lo pasamos a minuscula\n","filename = \"los3.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()\n","#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","#Sumarizamos los datos cargados\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total caracters: \", n_chars)\n","print(\"Total vocabulario: \", n_vocab)\n","#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","  seq_in = raw_text[i:i + seq_length]\n","  seq_out = raw_text[i + seq_length]\n","  dataX.append([char_to_int[char] for char in seq_in])\n","  dataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total patrones: \", n_patterns)\n","#Remodelar X para que sea [muestras, pasos de tiempo, características]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","#Normalizacion\n","X = X / float(n_vocab)\n","#Codificacion en caliente con la variable de salida\n","y = np_utils.to_categorical(dataY)\n","#Se define el LSTM modelo\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","#Se define el checkpoint\n","filepath=\"pesos-los3grandes-303-{epoch:02d}-{loss:.4f}.hdf5\";\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]\n","#Ajuste del modelo\n","model.fit(X, y, epochs=300, batch_size=64, callbacks=callbacks_list)"],"metadata":{"id":"ADuLGCKr_06y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La ejecución de este ejemplo lleva algún tiempo, dependiendo de tu ordenador. Después de ejecutar este ejemplo, se puede lograr una pérdida de aproximadamente 0.018. Por ejemplo, el mejor resultado que obtuve al ejecutar este modelo se almacenó en un archivo de punto de control con el nombre:"],"metadata":{"id":"Y3Id5kdv_3Od"}},{"cell_type":"code","source":["filename=\"pesos-los3grandes-303-150-0.0105.hdf5\""],"metadata":{"id":"gReZHk-C_4vw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como en el punto anterior, podemos utilizar este mejor modelo de la ejecución para generar texto. El único cambio que necesitamos hacer en el script de generación de texto del punto anterior está en la especificación de la topología de la red y desde qué archivo sembrar los pesos de la red. El código completo sería (con todos los pasos vistos):"],"metadata":{"id":"zqgtULjX_6hf"}},{"cell_type":"code","source":["#Carga de la red LSTM grande para generar texto\n","import sys\n","import numpy\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.utils import np_utils\n","#Cargamos el texto y lo pasamos a minuscula\n","filename = \"los3.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()\n","#Crear mapeo de caracteres únicos a enteros, y un mapeo inverso\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","#Sumarizamos los datos cargados\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)\n","#Preparar el conjunto de datos de entrada para los pares de salida codificados como enteros\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","    n_patterns = len(dataX)\n","\n","print(\"Total Patterns: \", n_patterns)\n","#Remodelar X para que sea [muestras, pasos de tiempo, características]\n","X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n","#Normalizacion\n","X = X / float(n_vocab)\n","#Codificacion en caliente de la variable de salida\n","y = np_utils.to_categorical(dataY)\n","#Define la LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(256))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","#Carga de los pesos de la red\n","filename=\"pesos-los3grandes-303-150-0.0105.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","#Toma una semilla aleatoria\n","start = numpy.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Semilla:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","#Genera los carácteres\n","for i in range(1000):\n","  x = numpy.reshape(pattern, (1, len(pattern), 1))\n","  x = x / float(n_vocab)\n","  prediction = model.predict(x, verbose=0)\n","  index = numpy.argmax(prediction)\n","  result = int_to_char[index]\n","  seq_in = [int_to_char[value] for value in pattern]\n","  sys.stdout.write(result)\n","  pattern.append(index)\n","  pattern = pattern[1:len(pattern)]\n","print(\"\\nHecho.\")"],"metadata":{"id":"og4jf7Vu__VS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Un ejemplo de ejecución de este script de generación de texto produce la salida a continuación. El texto semilla elegido al azar fue:\n","\n","Semilla: *\" l más pequeño-, la paja es blanda y se puede sujetar con facilidad. terminaré muy pronto y podré ir \"*\n","\n","El texto generado con la semilla (limpiado para presentación) fue:\n","\n","*a jugar. el hermano mediano decidió que su casa sería de madera: - puedo encontrar un montón de madera por los alrededores - explicó a sus hermanos, - construiré mi casa en un santiamén con todos estos troncos y me iré también a jugar. cuando las tres casitas estuvieron terminadas, los cerditos cantaban y bailaban en la puerta, felices por haber acabado con el problema: -¡quién teme al lobo feroz, al lobo, al lobo! - ¡quién teme al lobo feroz, al lobo feroz! - cantaban desde dentro los cerditos. el lobo estaba realmente enfadado y hambriento, y ahora deseaba comerse a los tres cerditos más que nunca, y frente a la puerta dijo: - ¡cerditos, abridme la puerta! - no, no, no, no te vamos a abrir. - pues si no me abrís... ¡soplaré y soplaré y la casita derribaré! y se puso a soplar tan fuerte como el viento de invierno. sopló y sopló, pero la casita de ladrillos era muy resistente y no conseguía derribarla. decidió trepar por la pared y entrar por la chimenea. se deslizó hacia abajo\n","Hecho.*\n","\n","Podemos ver que generalmente hay menos errores de ortografía y el texto parece más realista. Estos son mejores resultados, pero hay todavía se podría mejorar mucho más."],"metadata":{"id":"QwavCY_YACM0"}}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}