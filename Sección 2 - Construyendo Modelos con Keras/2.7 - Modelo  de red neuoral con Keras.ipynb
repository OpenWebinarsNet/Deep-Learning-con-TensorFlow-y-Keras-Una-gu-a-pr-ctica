{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+qeHBItj087ED6yM4u5Ty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Red Neuronal con Keras\n","Los pasos para poder crear nuestro modelo, son los siguientes:\n","\n","1. Cargar datos.\n","2. Definir tu Modelo.\n","3. Compilar el Modelo.\n","4. Entrenar el Modelo.\n","5. Evaluar el Modelo.\n","6. Unir todos los pasos anteriores."],"metadata":{"id":"WRIASf1mZZ3l"}},{"cell_type":"markdown","source":["## Datasets\n","\n","Vamos a utilizar uno de los datasets disponible por la Universidad de California Irvine, en este caso: **Diabetes de Indios Pima**. Este dataset es un conjunto de datos disponible de manera gratuita desde el repositorio UCI Machine Learning. Describe los datos de registros médicos de los pacientes y describe si ellos tuvieron diabetes dentro de un periodo de cinco años. Es un problema de clasificación binaria (aparición de diabetes como 1 o no como 0). Las variables de entrada que describen a cada paciente son numéricas y tienen escalas variables. A continuación se enumeran los ocho atributos para el conjunto de datos:\n","\n","1. Número de veces embarazada.\n","2. Concentración de glucosa plasmática a 2 horas en una prueba oral de tolerancia a la glucosa.\n","3. Presión arterial diastólica (mm Hg).\n","4. Grosor del pliegue de la piel del tríceps (mm).\n","5. Insulina en suero de 2 horas (mu U / ml).\n","6. Índice de masa corporal.\n","7. Función de pedigrí de diabetes.\n","8. Edad (años).\n","9. Clase, aparición de diabetes dentro de los cinco años.\n","\n","Debido a que todos los atributos son numericos, es facil utilizarlos directamente en la red neuronal. Este dataset contiene 768 registros.\n","La precisión base de todas las predicciones que se realizan es del 65.1%. Los mejores resultados se encuentran en el rango de 77.7% de precisión utilizando 10-fold cross validation.\n","\n","## Cargar datos\n","\n","En el siguiente código, se importan las librerias de Keras y numpy.\n","Debido a que todos los datos son numéricos, se puede importar el archivo directo con numpy.\n","También se inicia el generador aleatorio con la variable seed, para que siempre obtengamos los mismos resultados y no desviados."],"metadata":{"id":"gc8IR4-72uVP"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense\n","import numpy\n","\n","FILENAME = '../diabetes.csv'\n","\n","seed = 7\n","numpy.random.seed(seed)\n","\n","dataset = numpy.loadtxt(FILENAME, delimiter=',')\n","\n","training_data = dataset[:, 0:8]\n","training_targets = dataset[:, 8]"],"metadata":{"id":"JM64QaCXZ9gg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definir tu modelo\n","\n","Los modelos en Keras son definidos como una sequencias de capas. Podemos crear un modelo sequencia (**Sequencial**) y agregar capas una a una hasta que cumplan nuestros requerimientos.\n","\n","El primer paso es asegurarnos que la primer capa tenga el numero correcto de entradas. Estos puede ser especificado con el argumento **input_dim**, en este caso utilizaremos el valor de 8 debido a nuestras 8 variables de entrada.\n","\n","*¿Cómo saber el numero de capas y sus tipos?* Esta es una pregunta complicada pero normalmente se experimenta y a prueba y error se puede llegar a un resultado óptimo. En este caso vamos a utilizar una red conectada completamente con 3 capas.\n","\n","Capas completamente conectadas son definidas mediante la clase **Dense**. En esta clase se define el numero de neuronas como primer argumento, como segundo argumento se define el método de inicialización, y la función de activación. En este caso vamos a iniciar los pesos de la red con un valor aleatorio muy pequeño utilizando una distribución uniforme (uniform), en Keras el valor va de 0 a 0.05. Otra opción seria utilizar una distribución de Gauss (normal).\n","\n","En las primeras dos capas se definirá una función de activación relu y para la capa de salida una función sigmoid. En el pasado las funciones sigmoid y tanh eran las utilizadas en todas las capas, pero ha sido demostrado que el desempeño mejora utilizando la función rectifier como activación. En nuestro caso utilizamos la función sigmoid en la capada de salida para asegurarnos que el valor de salido oscila entre 0 y 1 y es mas fácil definir el resultado en la clasificación binaria."],"metadata":{"id":"77ynOkRG3er1"}},{"cell_type":"code","source":["model = Sequential()\n","model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n","model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))"],"metadata":{"id":"dy1R13GQ3yuO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compilar modelo\n","\n","Una vez que el modelo esta definido, se procede a **compilarlo**. Al compilar el modelo se mandan llamar las librerias en el backend, en nuestro caso Tensorflow. En este caso el backend automáticamente selecciona la mejor forma de representar la red neuronal para entrenamiento y realizar predicciones en el hardware. Cuando se compila el modelo, se deben definir algunas propiedades adicionales requeridas para el entrenamiento del modelo:\n","- Especifica la función de perdida o loss function, que es utilizada para evaluar los pesos.\n","- También debes definir el optimizador utilizado para buscar entre los pesos de la red y algunas métricas opcionales que se require colectar y reportar durante el entrenamiento.\n","En este ejemplo utilizaremos una función de perdida **binary_crossentropy**, que es una función logarítmica de perdida. Se utilizará una función para los calcular los gradientes llamada **adam**. Al ser un problema de clasificación binaria, se coleccionará y reportará la precision de la clasificación utilizando **accuracy**."],"metadata":{"id":"etzhCtYZ32zM"}},{"cell_type":"code","source":["model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"IMIw4UMe35eS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenar el modelo\n","\n","Una vez que este definido y compilado tu modelo, es tiempo de **ejecutar el modelo con los dato**s. Podemos entrenar nuestro modelo llamando la **función fit()** en el modelo.\n","\n","El proceso de entrenamiento se ejecuta cierto numero de veces utilizando el dataset, este numero de veces es llamado **epochs**, y se define utilizando el parámetro nb_epoch. También podemos definir el numero de instances que son evaluadas antes de que los pesos sean actualizados en la red neuronal. Este parámetro se llama batch_size. En este problema se definirá un numero pequeño de epochs (150) y un valor relativamente pequeño de el batch (10)."],"metadata":{"id":"fz8ImCSc38bU"}},{"cell_type":"code","source":["model.fit(training_data, training_targets, epochs=150, batch_size=10)"],"metadata":{"id":"9j4C9i7u3_NF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evalúa el modelo\n","\n","Ahora que hemos entrenado nuestro modelo utilizando el dataset completo, debemos evaluar el rendimiento de la red neuronal. Estos nos da una idea que tan bien modelamos nuestro dataset, pero no tendremos idea como se desempeñara el modelo con datos nuevos. En la realidad se debe de separar el dataset de training y el dataset de testing.\n","\n","Puedes evaluar tu modelo en tu dataset de training utilizando la función **evaluation**() de tu modelo, pasándole la misma entrada y salida utilizada cuando lo entrenaste. **Esto generará una predicción por cada registro, se podrán colectar scores, average loss y otras métricas como accuracy.**"],"metadata":{"id":"OWjSwasJ4CJk"}},{"cell_type":"code","source":["scores = model.evaluate(training_data, training_targets)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))"],"metadata":{"id":"RZwBRgjJ4E-W"},"execution_count":null,"outputs":[]}]}